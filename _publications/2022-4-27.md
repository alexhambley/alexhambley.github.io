---
title: "Optimising the website accessibility conformance evaluation methodology"
collection: publications
permalink: /publication/2022-4-27
# excerpt: 'The Website Accessibility Conformance Evaluation Methodology (WCAG-EM) is a methodology that guides an auditor through evaluating a website's conformance to the Web Content Accessibility Guidelines (WCAG). In this paper, we raise some concerns present in the five stages of the WCAG-EM, including the evaluative scope, the non-probabilistic sampling approach, and the potential for bias within the selected sample. We propose a novel framework positioned parallel to the WCAG-EM, but with additional statistical-based metrics to address these concerns. These metrics consider the coverage, representativeness, complexity, popularity, freshness, and accessibility of a population of pages. We provide an overview of these metrics and discuss the possible use with three web page population-sourcing methods: server log files, breadth-first crawling, and depth-first crawling. Finally, we highlight the potential of utilising server log files as a population-sourcing method.'
date: 2022-4-27
venue: 'Proceedings of the 19th International Web for All Conference'
paperurl: 'https://doi.org/10.1145/3493612.3520452'
citation: 'Alexander Hambley, Yeliz Yesilada, Markel Vigo, Simon Harper. 2022.  Optimising the website accessibility conformance evaluation methodology. Proceedings of the 19th International Web for All Conference.'
---
The Website Accessibility Conformance Evaluation Methodology (WCAG-EM) is a methodology that guides an auditor through evaluating a website's conformance to the Web Content Accessibility Guidelines (WCAG). In this paper, we raise some concerns present in the five stages of the WCAG-EM, including the evaluative scope, the non-probabilistic sampling approach, and the potential for bias within the selected sample. We propose a novel framework positioned parallel to the WCAG-EM, but with additional statistical-based metrics to address these concerns. These metrics consider the coverage, representativeness, complexity, popularity, freshness, and accessibility of a population of pages. We provide an overview of these metrics and discuss the possible use with three web page population-sourcing methods: server log files, breadth-first crawling, and depth-first crawling. Finally, we highlight the potential of utilising server log files as a population-sourcing method.

[Download paper here](https://doi.org/10.1145/3493612.3520452)